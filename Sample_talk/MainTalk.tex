\documentclass{beamer}

\mode<presentation>
{
  \usetheme{CambridgeUS}
  \setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage{xcolor,colortbl}
\usepackage[T1]{fontenc} 
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.
\usepackage{amsmath}

\newcommand{\linespace}{\vskip 0.25cm}

\definecolor{MyForestGreen}{rgb}{0,0.7,0} 
\newcommand{\tableemph}[1]{{#1}}
\newcommand{\tablewin}[1]{\tableemph{#1}}
\newcommand{\tablemid}[1]{\tableemph{#1}}
\newcommand{\tablelose}[1]{\tableemph{#1}}

\definecolor{MyLightGray}{rgb}{0.6,0.6,0.6}
\newcommand{\tabletie}[1]{\color{MyLightGray} {#1}}

% The text in square brackets is the short version of your title and will be used in the
% header/footer depending on your theme.
\title[CNNs in Medical Imaging]{Convolutional Neural Networks \\ in Medical Imaging}

% Sub-titles are optional - uncomment and edit the next line if you want one.
% \subtitle{Why does sub-tree crossover work?} 

% The text in square brackets is the short version of your name(s) and will be used in the
% header/footer depending on your theme.
\author[Finzel]{Mitchell Finzel}

% The text in square brackets is the short version of your institution and will be used in the
% header/footer depending on your theme.
\institute[U of Minn, Morris]
{
  Division of Science and Mathematics \\
  University of Minnesota, Morris \\
  Morris, Minnesota, USA
}

% The text in square brackets is the short version of the date if you need that.
\date[April '17] % (optional)
{15 April 2017, \\ Senior Seminar Conference}

% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline}
    \tableofcontents[currentsection, hideothersubsections]
  \end{frame}
}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% For a 20-25 minute senior seminar talk you probably want something like:
% - Two or three major sections (other than the summary).
% - At *most* three subsections per section.
% - Talk about 30s to 2min per frame. So there should probably be between
%   15 and 30 frames, all told.

\section*{Overview}

\begin{frame}
  \frametitle{Introduction}
  \tableofcontents[hideallsubsections]
  \begin{itemize}
  	\item Convolutional neural networks or CNNs, have seen a rise in popularity in image related fields.
  	\item CNNs have been having great success in biological segmentation tasks.
  	\item These tasks include:
  	\begin{itemize}
  	  \item The automated detection of lymph nodes
  	  \item Segmentation of knee cartilage
  	  \item Detection of Alzheimer's
  	\end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Introduction}
  \tableofcontents[hideallsubsections]
  \begin{itemize}
	\item We will be looking at two approaches to brain MRI segmentation
	\item The goal of this work is to provide unsegmented MRIs to the network and receive properly segmented MRIs as output
	\item Currently this requires time consuming labor from a skilled medical professional
  \end{itemize}
\end{frame}

\begin{frame} 
	\frametitle{Input Output Example}
    \includegraphics[width=0.75\textwidth]{"../Sample_paper/TBI Visuals".pdf}
    \\
    \only{Taken from~\cite{Kamnitsas:2017}}
\end{frame}

\begin{frame}
  \frametitle{Outline}
  \tableofcontents[hideallsubsections]
  \begin{itemize}
  	\item Background - Information about basic structural concepts for CNNs
  	\item A novel two pathway approach by Havaei, et al.
  	\item 3D multi-scale approach by Kamnitsas, et al.
  	\item Results
  	\item Conclusions
  \end{itemize}
\end{frame}

\subsection*{Background}

\begin{frame}
  \frametitle{Classification}
  \begin{itemize}
	\item Classification is the process of identifying something
	\item In the case of images we might classify something as an image of a brain versus an image of a foot
	\item The name of these classifications is often referred to as `labels'
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Biological Segmentation}
  \begin{itemize}
	\item Segmentation is the process of identifying the boundaries of different structures and classifying them
	\item Segmentation is loosely defined and can have a wide range of granularities
	\begin{itemize}
	  \item Rough grained, such as identifying the different bones in a leg X-ray
	  \item Fine grained, such as determining the differing regions of a tumor
	\end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Neural Networks}
  \begin{itemize}
	\item Neural Networks are a form of machine learning
	\item Neural Networks can be thought of as pattern recognizers.
	\item They are loosely based on the neuronal structure of the cerebral cortex, the part of the brain that takes in sensory data.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Neural Network Structure}
  \begin{itemize}
	\item Comprised of layers of nodes
	\item Each node has an activation function that triggers when it recognizes something in the input
	\item These activations are then passed to neighboring nodes through weighted connections eventually leading to some type of output
	\item The network can `learn' by altering the weights of its connections based on the accuracy of the output to the goal result
  \end{itemize}
\end{frame}

\begin{frame}
	\begin{center}
	  \includegraphics[width=0.4\textwidth]{"../Sample_paper/Neural Network".png}
	\end{center}
\end{frame}

\begin{frame}
  \frametitle{Kernels}
  
  \begin{itemize}
	\item Kernels, neurons and filters are interchangeable names
	\item Kernels are an array based representation of image features
	\item More kernels equals more recognizable features
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{Kernels}
   \includegraphics[width=0.95\textwidth]{Filter.png}
       \\
    \only{\tiny{Kernel \\ \url{https://adeshpande3.github.io/} }}
\end{frame}

\begin{frame}
  \frametitle{Convolutional Layers}
  \begin{itemize}
	\item Convolutional Layers are where CNNs get their name
	\item Every CNN starts with a convolutional layer
	\item The kernel slides or `convolves' around the input image
	\item The results of the convolutions are stored in the feature map
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Convolutional Layers}
   \includegraphics[width=0.95\textwidth]{ActivationMap.png}
       \\
    \only{\tiny{Feature Map \\ \url{https://adeshpande3.github.io/} }}
\end{frame}

\begin{frame}
  \frametitle{Fully Connected Layers} 
  \begin{itemize}
	\item Can be thought of as the final layers in the network
	\item Their job is to convert the feature maps from previous layers into label probabilities	
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Training}
  \begin{itemize}
	\item Training is the crux that makes everything work
	\item Training requires data that has already been properly segmented
	\item Network is initialized with random kernel weights
	\item Training has four main steps:
	\begin{itemize}
	  \item The forward pass
	  \item The loss calculation
	  \item The backward pass
	  \item Weight update
	\end{itemize}
	\item These four steps are performed on the entirety of the training data set multiple times
  \end{itemize}
\end{frame}

\section*{Methods}

\subsection*{Havaei, et al.}

\begin{frame}
  \frametitle{Overview of Havaei, et al.}
  \begin{itemize}
	\item Havaei, et al. proposes a two pathway approach to the BRATS 2013 brain tumor segmentation challenge
	\item Their approach has three main components
	\begin{itemize}
	  \item Two pathways
	  \item Two CNNs concatenated together
	  \item A two phase approach to training
	\end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Two Pathways}
  \begin{itemize}
	\item Havaei, et al. set up their network with two pathways
	\begin{itemize}
	  \item The local pathway with a smaller 7x7 pixel receptive field
	  \item The global pathway with a larger 13x13 pixel receptive field
	\end{itemize}
	\item These two pathways allow the combination of fine detail with greater locational context
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Two Pathways}
  \includegraphics[width=1\textwidth]{"../Sample_paper/Two-Pathway".pdf}
  \\
  \only{Taken from~\cite{Havaei:2017}}
\end{frame}

\begin{frame}<presentation:0>[noframenumbering]
  \frametitle{Cascaded Architectures}
  \begin{itemize}
	\item The second approach implemented by Havaei, et al. is their use of a `cascaded architecture'
	\item In this approach they take the output of one CNN and concatenate it into varying layers of another CNN
	\item The goal is to allow for joint segmentation models where different labels are allowed to correlate with one another
  \end{itemize}
\end{frame}

\begin{frame}<presentation:0>[noframenumbering]
  \frametitle{Cascaded Architectures}
  \includegraphics[width=1\textwidth]{"../Sample_paper/Two-Pathway".pdf}
  \\
  \only{Taken from~\cite{Havaei:2017}}
\end{frame}

\begin{frame}<presentation:0>[noframenumbering]
  \frametitle{Cascaded Architecture Results}
  \includegraphics[width=1\textwidth]{"CascadeResults".pdf}
  \\
  \only{\tiny{Taken from~\cite{Havaei:2017}}}
\end{frame}

\begin{frame}
  \frametitle{Two Phase Training}
  \begin{itemize}
	\item The last approach implemented by Havaei, et al. is a two phase training system
	\item This is done to alleviate the relative abundance of healthy tissue versus the small quantity of tumor tissue in each image
	\item The two phases consist of:
	\begin{itemize}
	  \item First they train the network on image patches where the probability of each label being present is equal
	  \item Then they retrain the final layer with the relative probabilities of each label
	\end{itemize}
	\item This allows for better label discrimination while maintaining proper output probabilities
  \end{itemize}
\end{frame}

\subsection*{Kamnitsas, et al.}

\begin{frame}
  \frametitle{Overview of Kamnitsas, et al.}
  \begin{itemize}
	\item Kamnitsas, et al have five different architecture approaches
	\begin{itemize}
	  \item<1,2> 3D kernels
	  \item<1> Dense training
	  \item<1,2> Two pathways
	  \item<1,2> Deeper networks
	  \item<1> 3D conditional random fields on the output
	\end{itemize}
	\item These approaches lead to top performances in three different brain related segmentation challenges
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{3D Kernels}
  \begin{itemize}
	\item 3D kernels can be thought of as 3 dimensional rectangular prisms
	\item Before the kernel convolved around a 2 dimensional space, but now it is convolving around a 3D space
	\item 3D kernels add to the computational costs
	\item Kamnitsas, et al. proposes a hybrid training scheme to resolve this
  \end{itemize}
\end{frame}

\begin{frame}<presentation:0>[noframenumbering]
  \frametitle{Dense Training}
  \begin{itemize}
	\item Normally when a feature map reaches the fully connected classifier layer it is of the same dimensions as the classifier's kernel so that the kernel perfectly overlaps the receptive field
	\item In dense training the feature map being sent to the classifier layer is larger than the receptive field
	\item In this situation a smaller 1x1x1 kernel is used to provide probabilities for each cell of the feature map independently
	\item This saves on computational cost by avoiding the use of overlapping image patches
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Two-Pathways}
  \begin{itemize}
	\item Much like Havaei, et al. Kamnitsas, et al. use a two pathway approach
	\item These two pathways are meant to capture global and local context
	\item Unlike Havaei, et al. they downsample the input image for one of the pathways rather than change the size of the receptive field
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Deeper Networks}
  \begin{itemize}
	\item Kamnitsas, et al. also explore the use of deeper neural networks
	\item A deeper network has more consecutive layers
	\item Deeper networks increase the discriminative capability of CNNs
	\item A drawback is the increase in trainable parameters
	\item Kamnitsas, et al. address this by decreasing the size of the kernels, thus lowering the number of trainable parameters
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Deeper Networks}
  \includegraphics[width=1\textwidth]{"Kamnitsas Basic Architecture".pdf}
  \\
  \only{Taken from~\cite{Kamnitsas:2017}}
\end{frame}

\begin{frame}<presentation:0>[noframenumbering]
  \frametitle{3D Conditional Random Fields}
  \begin{itemize}
	\item The final technique implemented by Kamnitsas, et al. is a 3D conditional random field
	\item This 3D CRF is used after the normal classification layer to better incorporate information from neighboring pixels in each pixels predictions
	\item The 3D CRF was shown to have statistically significant impact on their success in all three of the challenges they participated in
  \end{itemize}
\end{frame}

\section*{Results}

\begin{frame} 
	\frametitle{Havaei, et al. Results}
	\begin{table}
	\begin{tabular}{l | c | c | c }
	Name & Dice & Specificity & Sensitivity\\
	\hline
	\cellcolor{blue!25}InputCascadeCNN* & 0.84 & 0.88 & 0.84\\
	Tustison & 0.79 & 0.83 & 0.81\\
	Zhao & 0.79 & 0.77 & 0.85\\
	Meier & 0.72 & 0.65 & 0.88\\
	Reza & 0.73 & 0.68 & 0.79\\
	Cordier & 0.75 & 0.79 & 0.78
	\end{tabular}
	\caption{Comparison of Havaei, et al's. results on BRATS 2013 leaderboard set}
	\end{table}
\end{frame}

\begin{frame} 
	\frametitle{Kamnitsas, et al. Results}
	\begin{table}
	\begin{tabular}{l | c | c | c }
	Name & Dice & Precision & Sensitivity\\
	\hline
	\cellcolor{blue!25}Ensemble+CRF & 90.1 & 91.9 & 89.1\\
	\cellcolor{blue!25}Ensemble & 90.0 & 90.3 & 90.4\\
	\cellcolor{blue!25}DeepMedic+CRF & 89.8 & 91.5 & 89.1\\
	\cellcolor{blue!25}DeepMedic & 89.7 & 89.7 & 90.5\\
	bakas1 & 88 & 90 & 89\\
	peres1 & 87 & 89 & 86\\
	anon1 & 84 & 90 & 82\\
	thirs1 & 80 & 84 & 79\\
	peyrj & 80 & 87 & 77
	\end{tabular}
	\caption{Average performance of Kamnitsas, et al. on the training data from BRATS 2015 compared to other teams}
	\end{table}
\end{frame}

\begin{frame} 
	\frametitle{Traumatic Brain Injury Example}
    \includegraphics[width=0.75\textwidth]{"../Sample_paper/TBI Visuals".pdf}
    \\
    \only{Taken from~\cite{Kamnitsas:2017}}
\end{frame}

\begin{frame}
	\frametitle{Acknowledgements}
	I would like to thank Nic McPhee and Elena Machkasova for their guidance throughout this process. I would also like to thank my parents for their support.
\end{frame}

\begin{frame} 
	\frametitle{References}
	\bibliographystyle{amsalpha}
	\bibliography{../Sample_paper/main}
\end{frame}


\end{document}


